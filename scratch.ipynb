{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from constants import DEBUG_POSTGRESQL_PARQUET_FOLDER\n",
    "from pathlib import Path\n",
    "from scipy.sparse import dok_array\n",
    "import networkx as nx\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from ddsketch import DDSketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 empty query templates in /tmp/meowquet/postgres_0.csv.parquet.\n",
      "Encoding query templates.\n",
      "Computing think times.\n",
      "Updating transitions for sessions.\n",
      "Updating transitions for transactions.\n"
     ]
    }
   ],
   "source": [
    "class QueryTemplateEncoder:\n",
    "    \"\"\"\n",
    "    Why not sklearn.preprocessing.LabelEncoder()?\n",
    "\n",
    "    - Not all labels (query templates) are known ahead of time.\n",
    "    - Not that many query templates, so hopefully this isn't a bottleneck.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._encodings = {}\n",
    "        self._inverse = {}\n",
    "        self._next_label = 1\n",
    "\n",
    "    def fit(self, labels):\n",
    "        for label in labels:\n",
    "            if label not in self._encodings:\n",
    "                self._encodings[label] = self._next_label\n",
    "                self._inverse[self._next_label] = label\n",
    "                self._next_label += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, labels):\n",
    "        return [self._encodings[label] for label in labels]\n",
    "\n",
    "    def fit_transform(self, labels):\n",
    "        return self.fit(labels).transform(labels)\n",
    "\n",
    "    def inverse_transform(self, encodings):\n",
    "        return [self._inverse[encoding] for encoding in encodings]\n",
    "\n",
    "\n",
    "class QtMeta:\n",
    "    def __init__(self):\n",
    "        self._think_time_sketch = DDSketch()\n",
    "\n",
    "    def record(self, think_time):\n",
    "        self._think_time_sketch.add(think_time)\n",
    "\n",
    "class DfMeta:\n",
    "    SESSION_BEGIN = \"SESSION_BEGIN\"\n",
    "    SESSION_END = \"SESSION_END\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.qtms = {}\n",
    "        self.qt_enc = QueryTemplateEncoder()\n",
    "        # Dummy tokens for session begin and session end.\n",
    "        self.qt_enc.fit([self.SESSION_BEGIN, self.SESSION_END, pd.NA])\n",
    "\n",
    "        # networkx dict_of_dicts format.\n",
    "        self.transition_sessions = {}\n",
    "        self.transition_txns = {}\n",
    "\n",
    "    def augment(self, df):\n",
    "        # Augment the dataframe while updating internal state.\n",
    "\n",
    "        # Encode the query templates.\n",
    "        print(\"Encoding query templates.\")\n",
    "        df[\"query_template_enc\"] = self.qt_enc.fit_transform(df[\"query_template\"])\n",
    "\n",
    "        # Lagged time.\n",
    "        df[\"think_time\"] = (df[\"log_time\"] - df[\"log_time\"].shift(1)).shift(-1).dt.total_seconds()\n",
    "\n",
    "        def record_thinks(row):\n",
    "            qt_enc = row[\"query_template_enc\"]\n",
    "            think_time = row[\"think_time\"]\n",
    "            self.qtms[qt_enc] = self.qtms.get(qt_enc, QtMeta())\n",
    "            self.qtms[qt_enc].record(think_time)\n",
    "\n",
    "        print(\"Computing think times.\")\n",
    "        df.apply(record_thinks, axis=1)\n",
    "\n",
    "        print(\"Updating transitions for sessions.\")\n",
    "        self._update_transition_dict(self.transition_sessions, self._compute_transition_dict(\"session_id\"))\n",
    "        print(\"Updating transitions for transactions.\")\n",
    "        self._update_transition_dict(self.transition_txns, self._compute_transition_dict(\"virtual_transaction_id\"))\n",
    "\n",
    "    def visualize(self, target):\n",
    "        assert target in [\"sessions\", \"txns\"], f\"Bad target: {target}\"\n",
    "\n",
    "        if target == \"sessions\":\n",
    "            transitions = self.transition_sessions\n",
    "        else:\n",
    "            assert target == \"txns\"\n",
    "            transitions = self.transition_txns\n",
    "\n",
    "        def rewrite(s):\n",
    "            l = 24\n",
    "            return \"\\n\".join(s[i:i + l] for i in range(0, len(s), l))\n",
    "\n",
    "        G = nx.DiGraph(transitions)\n",
    "        nx.relabel_nodes(G, {k: rewrite(dfm.qt_enc.inverse_transform([k])[0]) for k in G.nodes}, copy=False)\n",
    "        AG = nx.drawing.nx_agraph.to_agraph(G)\n",
    "        AG.layout(\"dot\")\n",
    "        AG.draw(f\"{target}.pdf\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_transition_dict(current, other):\n",
    "        for src in other:\n",
    "            current[src] = current.get(src, {})\n",
    "            for dst in other[src]:\n",
    "                current[src][dst] = current[src].get(dst, {\"weight\": 0})\n",
    "                current[src][dst][\"weight\"] += other[src][dst][\"weight\"]\n",
    "                # Set the label for printing.\n",
    "                current[src][dst][\"label\"] = current[src][dst][\"weight\"]\n",
    "\n",
    "    def _compute_transition_dict(self, group_key):\n",
    "        assert group_key in [\"session_id\", \"virtual_transaction_id\"], f\"Unknown group key: {group_key}\"\n",
    "\n",
    "        group_fn = None\n",
    "        if group_key == \"session_id\":\n",
    "            group_fn = self._group_session\n",
    "        elif group_key == \"virtual_transaction_id\":\n",
    "            group_fn = self._group_txn\n",
    "        assert group_fn is not None, \"Forgot to add a case?\"\n",
    "\n",
    "        transitions = {}\n",
    "        groups = df.groupby(group_key)\n",
    "        chunksize = max(1, len(groups) // cpu_count())\n",
    "        grouped = process_map(group_fn, groups, chunksize=chunksize, desc=f\"Grouping on {group_key}.\", disable=True)\n",
    "        # TODO(WAN): Parallelize.\n",
    "        for group_id, group_qt_encs in tqdm(grouped, desc=f\"Computing transition matrix for {group_key}.\",\n",
    "                                            disable=True):\n",
    "            for transition in zip(group_qt_encs, group_qt_encs[1:]):\n",
    "                src, dst = transition\n",
    "                transitions[src] = transitions.get(src, {})\n",
    "                transitions[src][dst] = transitions[src].get(dst, {\"weight\": 0})\n",
    "                transitions[src][dst][\"weight\"] += 1\n",
    "                transitions[src][dst][\"label\"] = transitions[src][dst][\"weight\"]\n",
    "        return transitions\n",
    "\n",
    "    def _group_txn(self, item):\n",
    "        group_id, df = item\n",
    "        df = df.sort_values([\"log_time\", \"session_line_num\"])\n",
    "        qt_encs = df[\"query_template_enc\"].values\n",
    "        return group_id, qt_encs\n",
    "\n",
    "    def _group_session(self, item):\n",
    "        group_id, df = item\n",
    "        df = df.sort_values([\"log_time\", \"session_line_num\"])\n",
    "        qt_encs = df[\"query_template_enc\"].values\n",
    "        qt_encs = np.concatenate([\n",
    "            self.qt_enc.transform([self.SESSION_BEGIN]),\n",
    "            qt_encs,\n",
    "            self.qt_enc.transform([self.SESSION_END]),\n",
    "        ])\n",
    "        return group_id, qt_encs\n",
    "\n",
    "\n",
    "dfm = DfMeta()\n",
    "for pq_file in tqdm(sorted(list(Path(DEBUG_POSTGRESQL_PARQUET_FOLDER).glob(\"*.parquet\"))),\n",
    "                    desc=\"Reading Parquet files.\",\n",
    "                    disable=True):\n",
    "    df = pd.read_parquet(pq_file)\n",
    "    df[\"query_template\"] = df[\"query_template\"].replace(\"\", np.nan)\n",
    "    dropna_before = df.shape[0]\n",
    "    df = df.dropna(subset=[\"query_template\"])\n",
    "    dropna_after = df.shape[0]\n",
    "    print(f\"Dropped {dropna_before - dropna_after} empty query templates in {pq_file}.\")\n",
    "    dfm.augment(df)\n",
    "    break\n",
    "# dfm.visualize(\"sessions\")\n",
    "# dfm.visualize(\"txns\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# {dfm.qt_enc.inverse_transform([i])[0]: dfm.qtms[i]._think_time_sketch.get_quantile_value(0.5) for i in range(4,47)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "begin_times = df[df[\"query_template\"] == \"BEGIN\"].set_index(\"log_time\").resample(\"L\").size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49427, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                           ds  y\n0     2022-04-07 11:38:00.312  1\n1     2022-04-07 11:38:00.313  0\n2     2022-04-07 11:38:00.314  1\n3     2022-04-07 11:38:00.315  1\n4     2022-04-07 11:38:00.316  0\n...                       ... ..\n49422 2022-04-07 11:38:49.734  0\n49423 2022-04-07 11:38:49.735  0\n49424 2022-04-07 11:38:49.736  1\n49425 2022-04-07 11:38:49.737  0\n49426 2022-04-07 11:38:49.738  0\n\n[49427 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ds</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-04-07 11:38:00.312</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-04-07 11:38:00.313</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-04-07 11:38:00.314</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-04-07 11:38:00.315</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-04-07 11:38:00.316</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49422</th>\n      <td>2022-04-07 11:38:49.734</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49423</th>\n      <td>2022-04-07 11:38:49.735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49424</th>\n      <td>2022-04-07 11:38:49.736</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49425</th>\n      <td>2022-04-07 11:38:49.737</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49426</th>\n      <td>2022-04-07 11:38:49.738</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>49427 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ndf = begin_times.iloc[:-1].to_frame()\n",
    "ndf = ndf.tz_localize(None).reset_index().rename(columns={\"log_time\": \"ds\", 0: \"y\"})\n",
    "print(ndf.shape)\n",
    "display(ndf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.forecaster._handle_missing_data) - 47952 missing dates added.\n",
      "INFO - (NP.forecaster._handle_missing_data) - 997 NaN values in column y were auto-imputed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "More than 30 consecutive missing values encountered in column y. 46955 NA remain. Please preprocess data manually.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [81]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(num_points)\n\u001B[1;32m      8\u001B[0m m \u001B[38;5;241m=\u001B[39m NeuralProphet(n_forecasts\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, n_lags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mndf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m ndf_future \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mmake_future_dataframe(ndf, periods\u001B[38;5;241m=\u001B[39mnum_points\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m, n_historic_predictions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m pred_df \u001B[38;5;241m=\u001B[39m ndf_future\n",
      "File \u001B[0;32m~/.virtualenvs/default/lib/python3.8/site-packages/neuralprophet/forecaster.py:933\u001B[0m, in \u001B[0;36mNeuralProphet.fit\u001B[0;34m(self, df, freq, validation_df, epochs, local_modeling, progress_bar, plot_live_loss, progress_print, minimal)\u001B[0m\n\u001B[1;32m    931\u001B[0m     log\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel has already been fitted. Re-fitting will produce different results.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    932\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_dataframe(df, check_y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exogenous\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 933\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_missing_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_freq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    935\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m minimal:\n",
      "File \u001B[0;32m~/.virtualenvs/default/lib/python3.8/site-packages/neuralprophet/forecaster.py:412\u001B[0m, in \u001B[0;36mNeuralProphet.handle_missing_data\u001B[0;34m(self, df, freq, predicting)\u001B[0m\n\u001B[1;32m    410\u001B[0m df_handled_missing_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m()\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m df_list:\n\u001B[0;32m--> 412\u001B[0m     df_handled_missing_list\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_missing_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredicting\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    413\u001B[0m df \u001B[38;5;241m=\u001B[39m df_handled_missing_list\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m df\n",
      "File \u001B[0;32m~/.virtualenvs/default/lib/python3.8/site-packages/neuralprophet/forecaster.py:384\u001B[0m, in \u001B[0;36mNeuralProphet._handle_missing_data\u001B[0;34m(self, df, freq, predicting)\u001B[0m\n\u001B[1;32m    382\u001B[0m     log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m NaN values in column \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m were auto-imputed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(sum_na \u001B[38;5;241m-\u001B[39m remaining_na, column))\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m remaining_na \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 384\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    385\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMore than \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m consecutive missing values encountered in column \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    386\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m NA remain. Please preprocess data manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    387\u001B[0m                 \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimpute_limit_linear \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimpute_rolling, column, remaining_na\n\u001B[1;32m    388\u001B[0m             )\n\u001B[1;32m    389\u001B[0m         )\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# fail because set to not impute missing\u001B[39;00m\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    392\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing values found. \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease preprocess data manually or set impute_missing to True.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    393\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: More than 30 consecutive missing values encountered in column y. 46955 NA remain. Please preprocess data manually."
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet, set_random_seed\n",
    "\n",
    "set_random_seed(15721)\n",
    "\n",
    "num_points = ndf.shape[0]\n",
    "print(num_points)\n",
    "\n",
    "m = NeuralProphet(n_forecasts=1, n_lags=1)\n",
    "test = m.fit(ndf, freq=\"S\")\n",
    "ndf_future = m.make_future_dataframe(ndf, periods=num_points*2, n_historic_predictions=True)\n",
    "\n",
    "pred_df = ndf_future\n",
    "print(pred_df)\n",
    "\n",
    "nforecast = m.predict(pred_df)\n",
    "print(\"Forecast\")\n",
    "fig_forecast = m.plot(nforecast)\n",
    "plt.show()\n",
    "print(\"Components\")\n",
    "fig_components = m.plot_components(nforecast)\n",
    "plt.show()\n",
    "print(\"Params\")\n",
    "fig_model = m.plot_parameters()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G = G_old\n",
    "\n",
    "while True:\n",
    "    deg_out_one = set([node for node, degree in G.out_degree if degree == 1])\n",
    "    deg_in_one = set([node for node, degree in G.in_degree if degree == 1])\n",
    "    contraction_candidates = list(deg_in_one & deg_out_one)\n",
    "\n",
    "    for node in contraction_candidates:\n",
    "        pred = list(G.predecessors(node))[0]\n",
    "        succ = list(G.successors(node))[0]\n",
    "        predw = G.in_edges[pred, node][\"weight\"]\n",
    "        succw = G.out_edges[node, succ][\"weight\"]\n",
    "\n",
    "        # Try to merge the node with its successor.\n",
    "        if succ in deg_out_one and predw == succw:\n",
    "            nx.contracted_nodes(G, node, succ, self_loops=False, copy=False)\n",
    "            nx.relabel_nodes(G, {node: f\"{node},{succ}\"}, copy=False)\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "print(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 36))\n",
    "# pos = nx.nx_agraph.graphviz_layout(G)\n",
    "pos = nx.spring_layout(G, weight=None)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "labels = nx.get_edge_attributes(G, \"weight\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.savefig(\"sessions_contracted.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}