{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from constants import DEBUG_POSTGRESQL_PARQUET_FOLDER\n",
    "from pathlib import Path\n",
    "from scipy.sparse import dok_array\n",
    "import networkx as nx\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QueryTemplateEncoder:\n",
    "    \"\"\"\n",
    "    Why not sklearn.preprocessing.LabelEncoder()?\n",
    "\n",
    "    - Not all labels (query templates) are known ahead of time.\n",
    "    - Not that many query templates, so hopefully this isn't a bottleneck.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._encodings = {}\n",
    "        self._inverse = {}\n",
    "        self._next_label = 1\n",
    "\n",
    "    def fit(self, labels):\n",
    "        for label in labels:\n",
    "            if label not in self._encodings:\n",
    "                self._encodings[label] = self._next_label\n",
    "                self._inverse[self._next_label] = label\n",
    "                self._next_label += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, labels):\n",
    "        return [self._encodings[label] for label in labels]\n",
    "\n",
    "    def fit_transform(self, labels):\n",
    "        return self.fit(labels).transform(labels)\n",
    "\n",
    "    def inverse_transform(self, encodings):\n",
    "        return [self._inverse[encoding] for encoding in encodings]\n",
    "\n",
    "\n",
    "class DfMeta:\n",
    "    SESSION_BEGIN = \"SESSION_BEGIN\"\n",
    "    SESSION_END = \"SESSION_END\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.qt_enc = QueryTemplateEncoder()\n",
    "        # Dummy tokens for session begin and session end.\n",
    "        self.qt_enc.fit([self.SESSION_BEGIN, self.SESSION_END, pd.NA])\n",
    "\n",
    "        # networkx dict_of_dicts format.\n",
    "        self.transition_sessions = {}\n",
    "\n",
    "    def augment(self, df):\n",
    "        # Augment the dataframe while updating internal state.\n",
    "\n",
    "        # Encode the query templates.\n",
    "        df[\"query_template_enc\"] = self.qt_enc.fit_transform(df[\"query_template\"])\n",
    "\n",
    "        self._update_transition_dict(self.transition_sessions, self._compute_transition_dict(\"session_id\"))\n",
    "        # In a world of autocommit, this doesn't matter as much.\n",
    "        # self._update_transition_dict(self.transition_sessions, self._compute_transition_dict(\"virtual_transaction_id\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_transition_dict(current, other):\n",
    "        for src in other:\n",
    "            current[src] = current.get(src, {})\n",
    "            for dst in other[src]:\n",
    "                current[src][dst] = current[src].get(dst, {\"weight\": 0})\n",
    "                current[src][dst][\"weight\"] += other[src][dst][\"weight\"]\n",
    "\n",
    "    def _compute_transition_dict(self, group_key):\n",
    "        assert group_key in [\"session_id\", \"virtual_transaction_id\"], f\"Unknown group key: {group_key}\"\n",
    "\n",
    "        group_fn = None\n",
    "        if group_key == \"session_id\":\n",
    "            group_fn = self._group_session\n",
    "        elif group_key == \"virtual_transaction_id\":\n",
    "            group_fn = self._group_txn\n",
    "        assert group_fn is not None, \"Forgot to add a case?\"\n",
    "\n",
    "        transitions = {}\n",
    "        # grouped = [group_fn(item) for item in df.groupby(group_key)]\n",
    "        groups = df.groupby(group_key)\n",
    "        chunksize = len(groups) // cpu_count()\n",
    "        grouped = process_map(group_fn, groups, chunksize=chunksize, desc=f\"Grouping on {group_key}.\", disable=True)\n",
    "        # TODO(WAN): Parallelize.\n",
    "        for group_id, group_qt_encs in tqdm(grouped, desc=f\"Computing transition matrix for {group_key}.\",\n",
    "                                            disable=True):\n",
    "            for transition in zip(group_qt_encs, group_qt_encs[1:]):\n",
    "                src, dst = transition\n",
    "                transitions[src] = transitions.get(src, {})\n",
    "                transitions[src][dst] = transitions[src].get(dst, {\"weight\": 0})\n",
    "                transitions[src][dst][\"weight\"] += 1\n",
    "        return transitions\n",
    "\n",
    "    def _group_txn(self, item):\n",
    "        group_id, df = item\n",
    "        df = df.sort_values([\"log_time\", \"session_line_num\"])\n",
    "        group_vals = df[\"query_template_enc\"].values\n",
    "        return group_id, group_vals\n",
    "\n",
    "    def _group_session(self, item):\n",
    "        group_id, df = item\n",
    "        df = df.sort_values([\"log_time\", \"session_line_num\"])\n",
    "        group_vals = df[\"query_template_enc\"].values\n",
    "        group_vals = np.concatenate([\n",
    "            self.qt_enc.transform([self.SESSION_BEGIN]),\n",
    "            group_vals,\n",
    "            self.qt_enc.transform([self.SESSION_END]),\n",
    "        ])\n",
    "        return group_id, group_vals\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _compute_graph(transition_dict):\n",
    "    #     dd = {}\n",
    "    #     for src in transition_dict:\n",
    "    #         for dst in transition_dict[src]:\n",
    "    #             if dst == \"count\":\n",
    "    #                 continue\n",
    "    #             dd[src] = dd.get(src, {})\n",
    "    #             dd[src][dst] = transition_dict[src][dst] / transition_dict[src][\"count\"]\n",
    "    #         print(dd)\n",
    "    #         raise Exception\n",
    "    #     return nx.from_dict_of_dicts(dd)\n",
    "\n",
    "    # def _build_markov_chain(self, group):\n",
    "    #     assert group in [\"session_id\", \"virtual_transaction_id\"], \"What are you grouping by?\"\n",
    "    #     grouped = self.df.groupby(group)\n",
    "    #     group_vals = np.concatenate(\n",
    "    #         self.qt_enc.inverse_transform([self.SESSION_BEGIN])[0],\n",
    "    #         grouped.values,\n",
    "    #         self.qt_enc.inverse_transform([self.SESSION_END])[0])\n",
    "\n",
    "\n",
    "dfm = DfMeta()\n",
    "for pq_file in tqdm(sorted(list(Path(DEBUG_POSTGRESQL_PARQUET_FOLDER).glob(\"*.parquet\"))),\n",
    "                    desc=\"Reading Parquet files.\",\n",
    "                    disable=True):\n",
    "    df = pd.read_parquet(pq_file)\n",
    "    df[\"query_template\"] = df[\"query_template\"].replace(\"\", np.nan)\n",
    "    dropna_before = df.shape[0]\n",
    "    df = df.dropna(subset=[\"query_template\"])\n",
    "    dropna_after = df.shape[0]\n",
    "    print(f\"Dropped {dropna_before - dropna_after} empty query templates in {pq_file}.\")\n",
    "    dfm.augment(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfm.qt_enc.inverse_transform([257])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "G = nx.DiGraph(dfm.transition_sessions)\n",
    "fig = plt.figure(figsize=(24, 36))\n",
    "pos = nx.nx_agraph.graphviz_layout(G)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "labels = nx.get_edge_attributes(G, \"weight\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.savefig(\"sessions.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G_old = G"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# num_qt = df[\"query_template\"].nunique()\n",
    "# S = dok_array((num_qt, num_qt), dtype=np.int64)\n",
    "# S[1, 2] = 3\n",
    "# S[2, 3] = 3\n",
    "# S[1, 5] = 4\n",
    "# print(num_qt)\n",
    "# print(S)\n",
    "# G = nx.from_scipy_sparse_array(S, parallel_edges=False, edge_attribute=\"counts\")\n",
    "# print(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "G = G_old\n",
    "\n",
    "while True:\n",
    "    deg_out_one = set([node for node, degree in G.out_degree if degree == 1])\n",
    "    deg_in_one = set([node for node, degree in G.in_degree if degree == 1])\n",
    "    contraction_candidates = list(deg_in_one & deg_out_one)\n",
    "\n",
    "    for node in contraction_candidates:\n",
    "        pred = list(G.predecessors(node))[0]\n",
    "        succ = list(G.successors(node))[0]\n",
    "        predw = G.in_edges[pred, node][\"weight\"]\n",
    "        succw = G.out_edges[node, succ][\"weight\"]\n",
    "\n",
    "        # Try to merge the node with its successor.\n",
    "        if succ in deg_out_one and predw == succw:\n",
    "            nx.contracted_nodes(G, node, succ, self_loops=False, copy=False)\n",
    "            nx.relabel_nodes(G, {node: f\"{node},{succ}\"}, copy=False)\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "print(G)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 36))\n",
    "# pos = nx.nx_agraph.graphviz_layout(G)\n",
    "pos = nx.spring_layout(G, weight=None)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "labels = nx.get_edge_attributes(G, \"weight\")\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.savefig(\"sessions_contracted.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}